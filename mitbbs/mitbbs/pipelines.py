import os
import json
import numpy as np
import faiss
from sentence_transformers import SentenceTransformer

class FaissPipeline:
    def __init__(self):
        self.index_file = "forum_posts_index.faiss"
        self.text_file = "forum_posts_texts.json"
        self.link_file = "forum_posts_links.json"  # âœ… æ–°å¢ï¼šé“¾æ¥å»é‡ç”¨
        self.embed_model = SentenceTransformer('all-MiniLM-L6-v2')

        self.new_texts = []
        self.new_links = []

        # åŠ è½½æ—§é“¾æ¥å’Œæ–‡æœ¬
        if os.path.exists(self.link_file):
            with open(self.link_file, "r", encoding='utf-8') as f:
                self.existing_links = set(json.load(f))
        else:
            self.existing_links = set()

        if os.path.exists(self.index_file) and os.path.exists(self.text_file):
            self.index = faiss.read_index(self.index_file)
            with open(self.text_file, "r", encoding='utf-8') as f:
                self.all_texts = json.load(f)
        else:
            self.index = None
            self.all_texts = []

    def process_item(self, item, spider):
        link = item['link']
        if link in self.existing_links:
            spider.logger.info(f"â›” è·³è¿‡é‡å¤å¸–å­: {link}")
            return item  # ä¸å¤„ç†é‡å¤

        text = f"{item['title']}ã€‚\n{item['content']}"
        self.new_texts.append(text)
        self.new_links.append(link)
        return item

    def close_spider(self, spider):
        if not self.new_texts:
            print("âš ï¸ æ²¡æœ‰æ–°å¸–å­è¦å¤„ç†")
            return

        print(f"\nğŸ” æ­£åœ¨åµŒå…¥ {len(self.new_texts)} æ¡æ–°æ–‡æœ¬...")
        embeddings = self.embed_model.encode(self.new_texts, show_progress_bar=True)
        embeddings = np.array(embeddings)

        if self.index is None:
            dim = embeddings.shape[1]
            self.index = faiss.IndexFlatL2(dim)
        self.index.add(embeddings)

        self.all_texts.extend(self.new_texts)
        self.existing_links.update(self.new_links)

        # ä¿å­˜
        faiss.write_index(self.index, self.index_file)
        with open(self.text_file, "w", encoding="utf-8") as f:
            json.dump(self.all_texts, f, ensure_ascii=False, indent=2)
        with open(self.link_file, "w", encoding="utf-8") as f:
            json.dump(list(self.existing_links), f, ensure_ascii=False, indent=2)

        print(f"\nâœ… å‘é‡åº“æ›´æ–°å®Œæˆï¼šå…± {len(self.all_texts)} æ¡æ–‡æœ¬ï¼Œ{len(self.existing_links)} ä¸ªå”¯ä¸€é“¾æ¥")
