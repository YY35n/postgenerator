import os
import re
import time
import json
import requests
import faiss
import numpy as np
from bs4 import BeautifulSoup
from sentence_transformers import SentenceTransformer

BASE_URL = "https://www.1point3acres.com/bbs/"
FORUM_ID = 28  # forum-28 å¼€å¤´çš„ç‰ˆå—
START_PAGE = 1
END_PAGE = 5   # âœ… æƒ³çˆ¬å¤šå°‘é¡µè‡ªå·±æ”¹ï¼Œæ¯”å¦‚åˆ° forum-28-5.html
HEADERS = {
    "User-Agent": "Mozilla/5.0",
    "Referer": "https://www.1point3acres.com/bbs/forum-28-1.html"
}

# å¹¿å‘Šæç¤ºæ¸…å•
AD_PATTERNS = [
    r'^æ³¨å†Œä¸€äº©ä¸‰åˆ†åœ°è®ºå›.*?æ³¨å†Œè´¦å·x',
    r'^This post was last edited.*?on \d{4}-\d{1,2}-\d{1,2}',
    r'^æ‚¨éœ€è¦ç™»å½•æ‰å¯ä»¥ä¸‹è½½æˆ–æŸ¥çœ‹é™„ä»¶ã€‚æ²¡æœ‰å¸å·ï¼Ÿæ³¨å†Œè´¦å·x',
]

def clean_text(text):
    """ç§»é™¤å¼€å¤´çš„å¹¿å‘Šæ®µè½"""
    for pattern in AD_PATTERNS:
        text = re.sub(pattern, '', text)
    return text.strip()

def get_threads_from_forum(forum_url):
    """æ­£ç¡®æŠ“å–æ¯ä¸ªå¸–å­æ ‡é¢˜å’Œé“¾æ¥"""
    res = requests.get(forum_url, headers=HEADERS)
    soup = BeautifulSoup(res.text, 'html.parser')
    
    threads = []
    for tbody in soup.find_all('tbody', id=lambda x: x and x.startswith('normalthread_')):
        a_tag = tbody.select_one('th a.s.xst')
        if a_tag:
            title = a_tag.get_text(strip=True)
            link = BASE_URL + a_tag['href']
            threads.append({'title': title, 'link': link})
    return threads



# âœ… ä¸»æµç¨‹å¼€å§‹
new_data = []

for page_num in range(START_PAGE, END_PAGE + 1):
    forum_url = f"{BASE_URL}forum-{FORUM_ID}-{page_num}.html"
    print(f"\nğŸŒ æ­£åœ¨æŠ“å–ç¬¬ {page_num} é¡µï¼š{forum_url}")
    threads = get_threads_from_forum(forum_url)
    print(f"  æ‰¾åˆ° {len(threads)} ä¸ªå¸–å­")

    for idx, thread in enumerate(threads, 1):
        print(f"    [{idx}/{len(threads)}] æŠ“å–: {thread['title']}")
        content = get_threads_from_forum(thread['link'])
        new_data.append({
            'title': thread['title'],
            'link': thread['link'],
            'content': content or "ï¼ˆæœªèƒ½è·å–ä¸»è´´å†…å®¹ï¼‰"
        })
        time.sleep(1)

# âœ… åˆå¹¶æ—§æ•°æ®
if os.path.exists("threads_cleaned.json"):
    with open("threads_cleaned.json", "r", encoding='utf-8') as f:
        old_data = json.load(f)
    all_data = old_data + new_data
else:
    all_data = new_data

# ä¿å­˜JSON
with open("threads_cleaned.json", "w", encoding='utf-8') as f:
    json.dump(all_data, f, ensure_ascii=False, indent=2)

print(f"\nâœ… å½“å‰æ€»å…± {len(all_data)} æ¡å¸–å­ï¼Œå·²ä¿å­˜åˆ° threads_cleaned.json")

# âœ… åµŒå…¥å¹¶å¢é‡ä¿å­˜ FAISS
print("âœ¨ æ­£åœ¨å°†æ–°å¢å¸–å­è½¬åŒ–ä¸ºå‘é‡å¹¶åˆå¹¶åˆ°FAISSæ•°æ®åº“...")

# åŠ è½½åµŒå…¥æ¨¡å‹
embed_model = SentenceTransformer('all-MiniLM-L6-v2')

# æ–‡æœ¬ç»„åˆï¼ˆæ–°æŠ“åˆ°çš„ï¼‰
new_texts = [f"{item['title']}ã€‚\n{item['content']}" for item in new_data]
new_embeddings = embed_model.encode(new_texts, show_progress_bar=True)

# å¤„ç†FAISS
if os.path.exists("forum_posts_index.faiss") and os.path.exists("forum_posts_texts.json"):
    print("ğŸ”„ æ£€æµ‹åˆ°å·²æœ‰æ—§æ•°æ®åº“ï¼Œæ­£åœ¨å¢é‡åˆå¹¶...")
    index = faiss.read_index("forum_posts_index.faiss")
    with open("forum_posts_texts.json", "r", encoding='utf-8') as f:
        old_texts = json.load(f)
    all_texts = old_texts + new_texts
else:
    print("ğŸ†• æ²¡æœ‰æ—§æ•°æ®åº“ï¼Œæ–°å»º...")
    dimension = new_embeddings.shape[1]
    index = faiss.IndexFlatL2(dimension)
    all_texts = new_texts

index.add(np.array(new_embeddings))

# ä¿å­˜
faiss.write_index(index, "forum_posts_index.faiss")
with open("forum_posts_texts.json", "w", encoding='utf-8') as f:
    json.dump(all_texts, f, ensure_ascii=False, indent=2)

print(f"\nâœ… å½“å‰æ€»å…± {len(all_texts)} æ¡æ–‡æœ¬ï¼Œå‘é‡åº“å·²æ›´æ–°å®Œæˆï¼")
